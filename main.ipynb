{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830284a2",
   "metadata": {},
   "source": [
    "## Setup dell'Ambiente e Ingestione Dati\n",
    "\n",
    "In questa fase iniziale vengono importate le librerie necessarie per l'intero ciclo di vita del progetto e viene caricato il dataset grezzo delle traiettorie.\n",
    "\n",
    "**1. Importazione delle dipendenze**\n",
    "Il codice importa gli strumenti fondamentali suddivisi per funzionalità:\n",
    "* **Manipolazione Dati:** `pandas` e `numpy` per la gestione di strutture dati tabulari e operazioni matriciali efficienti.\n",
    "* **Deep Learning:** `tensorflow` e `tensorflow.keras` (inclusi `layers` e `Model`) costituiscono il framework per costruire le architetture neurali (RNN e Transformer).\n",
    "* **Preprocessing:** `sklearn` (Scikit-learn) fornisce utility essenziali per:\n",
    "    * `train_test_split`: dividere i dati in training e validation set.\n",
    "    * `MinMaxScaler`: normalizzare i dati, passaggio cruciale per la convergenza delle reti neurali.\n",
    "* **Visualizzazione:** `matplotlib.pyplot` per graficare le loss curves e i risultati delle predizioni.\n",
    "\n",
    "**2. Caricamento del Dataset**\n",
    "Viene definito il percorso del file e letto il CSV tramite `pd.read_csv`.\n",
    "* **`header=None`**: Indica che la prima riga del file **non** contiene i nomi delle colonne, ma è già parte dei dati numerici. Pandas assegnerà indici numerici automatici (0, 1, 2...) alle colonne.\n",
    "* **`index_col=False`**: Forza Pandas a non utilizzare la prima colonna come indice del DataFrame, che contiene i tempi.\n",
    "\n",
    "Infine, `df.shape` restituisce una tupla `(n_righe, n_colonne)` per una verifica immediata della dimensionalità dei dati caricati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T08:40:03.962334700Z",
     "start_time": "2026-01-20T08:39:58.509585600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Caricamento del Dataset\n",
    "FILE_PATH = '../data/trajectories.csv'\n",
    "\n",
    "# Leggiamo senza header perché il file contiene solo numeri\n",
    "df = pd.read_csv(FILE_PATH, header=None, index_col=False)\n",
    "\n",
    "print(f\"Dimensione totale dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da977dbd",
   "metadata": {},
   "source": [
    "## Pre-elaborazione: Suddivisione delle Traiettorie\n",
    "\n",
    "In questa sezione viene definita e utilizzata la funzione `split_trajectories`. Poiché il dataset originale contiene diverse simulazioni concatenate sequenzialmente, è necessario separarle per poter addestrare correttamente i modelli (RNN e Transformer) su sequenze indipendenti.\n",
    "\n",
    "La logica della funzione è la seguente:\n",
    "\n",
    "1.  **Rilevamento dei reset temporali**: La funzione isola la prima colonna (il tempo). Utilizzando `np.diff`, individua gli indici in cui il valore del tempo decresce (il passaggio da un tempo finale $t_{end}$ a un tempo iniziale $t_0$), che segnala l'inizio di una nuova traiettoria.\n",
    "2.  **Splitting**: Il DataFrame viene diviso in una lista di array `numpy` in corrispondenza di questi punti di taglio.\n",
    "3.  **Feature Selection**: Da ogni traiettoria viene rimossa la colonna del tempo (indice 0), mantenendo solo le feature fisiche (dalla colonna 1 in poi) che serviranno come input per la rete neurale.\n",
    "\n",
    "Infine, viene eseguito lo split e vengono stampate le statistiche di base (numero di traiettorie e dimensione) per verifica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bbf7cf18268f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T08:40:06.006358200Z",
     "start_time": "2026-01-20T08:40:05.971949300Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_trajectories(df):\n",
    "    trajectories = []\n",
    "    \n",
    "    # Identifichiamo dove il tempo ricomincia da capo (o è 0)\n",
    "    # Se il tempo al passo t è minore del tempo al passo t-1, è una nuova traiettoria\n",
    "    time_col = df.iloc[:, 0].values\n",
    "\n",
    "    # Troviamo gli indici dove inizia una nuova traiettoria\n",
    "    split_indices = np.where(np.diff(time_col) < 0)[0] + 1\n",
    "\n",
    "    # Splittiamo il dataframe in questi punti\n",
    "    traj_list = np.split(df.values, split_indices)\n",
    "\n",
    "    # Rimuoviamo la colonna del tempo (col 0) dalle feature per il training\n",
    "    # Teniamo solo le 55 colonne fisiche\n",
    "    clean_trajectories = [t[:, 1:] for t in traj_list]\n",
    "\n",
    "    return clean_trajectories\n",
    "\n",
    "# Eseguiamo lo split\n",
    "all_trajectories = split_trajectories(df)\n",
    "print(f\"Trovate {len(all_trajectories)} traiettorie distinte.\")\n",
    "print(f\"Shape di una traiettoria: {all_trajectories[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99343113",
   "metadata": {},
   "source": [
    "## Split del Dataset e Normalizzazione\n",
    "\n",
    "In questo blocco avvengono la separazione dei dati e la successiva normalizzazione.\n",
    "\n",
    "**1. Divisione Train/Test**\n",
    "Si utilizza `train_test_split` per dividere l'insieme delle traiettorie (`all_trajectories`) in due subset:\n",
    "* **Training Set (80%):** Usato per l'addestramento dei pesi del modello.\n",
    "* **Test Set (20%):** Usato esclusivamente per la valutazione finale delle performance su dati non visti.\n",
    "* `random_state=42`: Garantisce la riproducibilità dell'esperimento fissando il seed del generatore pseudo-casuale.\n",
    "\n",
    "**2. Normalizzazione (MinMax Scaling)**\n",
    "Viene applicato un `MinMaxScaler` per scalare i dati nel range **[-1, 1]**, così da centrare i dati attorno allo zero, facilitando l'ottimizzazione e prevenendo problemi di gradienti.\n",
    "\n",
    "**Prevenzione del Data Leakage**\n",
    "Il codice rispetta rigorosamente la separazione tra train e test:\n",
    "1.  **Fit solo sul Train:** Lo scaler calcola i valori minimi e massimi globali utilizzando **solo** i dati di training (`train_concat`). \n",
    "2.  **Transform:** I parametri appresi dal training vengono poi applicati per trasformare sia le traiettorie di training che quelle di test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd79c2737ead7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T08:40:10.270459900Z",
     "start_time": "2026-01-20T08:40:10.035261200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dividiamo le traiettorie in Train (80%) e Test (20%)\n",
    "train_traj, test_traj = train_test_split(all_trajectories, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizzazione (MinMax tra 0 e 1 o -1 e 1 è standard per dati quantistici)\n",
    "# Fittiamo lo scaler SOLO sul training set per correttezza metodologica\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Concateniamo temporaneamente per fittare lo scaler\n",
    "train_concat = np.vstack(train_traj)\n",
    "scaler.fit(train_concat)\n",
    "\n",
    "# Applichiamo la trasformazione a ogni singola traiettoria\n",
    "train_traj_norm = [scaler.transform(t) for t in train_traj]\n",
    "test_traj_norm = [scaler.transform(t) for t in test_traj]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512bfc5",
   "metadata": {},
   "source": [
    "## Generazione del Dataset (Sliding Windows)\n",
    "\n",
    "Questa funzione trasforma la lista di traiettorie grezze in un dataset strutturato per l'apprendimento supervisionato, utilizzando la tecnica delle **sliding windows**.\n",
    "\n",
    "La funzione `create_dataset` opera come segue:\n",
    "\n",
    "1.  **Iterazione per Traiettoria**: Itera su ogni singola simulazione (`trajectories`) separatamente. Questo serve per evitare che la fine di una simulazione venga usata per predire l'inizio di quella successiva.\n",
    "2.  **Creazione Input ($X$)**: Estrae una finestra temporale di lunghezza `input_width`. Questa rappresenta il contesto storico che il modello osserverà.\n",
    "3.  **Creazione Target ($y$)**: Estrae i passi temporali immediatamente successivi alla finestra di input, di lunghezza `forecast_horizon` (di default 1). Questo è ciò che il modello deve imparare a predire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699938dd7aabe2f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T08:45:12.682968200Z",
     "start_time": "2026-01-20T08:45:12.676239500Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(trajectories, input_width, forecast_horizon=1):\n",
    "    X, y = [], []\n",
    "    for t in trajectories:\n",
    "        for i in range(len(t) - input_width - forecast_horizon + 1):\n",
    "            X.append(t[i : i + input_width])\n",
    "            y.append(t[i + input_width : i + input_width + forecast_horizon])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77b4b3",
   "metadata": {},
   "source": [
    "## Definizione dell'Architettura RNN (QuantumRNN)\n",
    "\n",
    "In questo blocco viene definita la classe del modello neurale basata su unità ricorrenti **GRU**. Il modello è implementato tramite *subclassing* di `tf.keras.Model`, offrendo maggiore flessibilità e controllo sul flusso dei dati.\n",
    "\n",
    "**Componenti dell'Architettura**\n",
    "\n",
    "1.  **Layer Normalization:**\n",
    "    * Applicata direttamente all'input. Normalizza le feature per ogni singolo campione indipendentemente dal batch. Serve per stabilizzare i gradienti e accelerare la convergenza.\n",
    "\n",
    "2.  **Stacked GRU Layers:**\n",
    "    * Vengono utilizzate due celle GRU in sequenza. Le GRU sono state scelte per la loro efficienza computazionale rispetto alle LSTM, pur risolvendo efficacemente il problema del *vanishing gradient*.\n",
    "    * **GRU 1 (`return_sequences=True`):** Restituisce l'intera sequenza di output (un vettore per ogni step temporale). \n",
    "    * **GRU 2:** Restituisce solo l'**ultimo stato nascosto**. Questa configurazione implementa un'architettura **Many-to-One**, comprimendo l'informazione dell'intera sequenza storica in un unico vettore rappresentativo finale.\n",
    "\n",
    "3.  **Regolarizzazione (Dropout):**\n",
    "    * Vengono applicati due tipi di dropout per combattere l'overfitting:\n",
    "        * `dropout`: Maschera casualmente gli input del layer.\n",
    "        * `recurrent_dropout`: Maschera le connessioni ricorrenti (stato-stato), fondamentale per regolarizzare la memoria a lungo termine.\n",
    "\n",
    "4.  **Output Layer (Dense):**\n",
    "    * Un layer `Dense` finale senza funzione di attivazione (lineare) che proietta lo stato latente della GRU nella dimensione dell'output desiderato (`output_dim`, in questo caso 56 parametri per la regressione).\n",
    "\n",
    "**Il Metodo `call` (Forward Pass)**\n",
    "Definisce il flusso dei dati: `Input -> Norm -> GRU1 -> GRU2 -> Dense`.\n",
    "* **Gestione del flag `training`:** Il parametro viene passato esplicitamente ai layer GRU. Questo assicura che il Dropout sia attivo **solo** durante il training (`model.fit`) e disattivato automaticamente durante la validazione o l'inferenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed283713a30688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T08:45:14.385321900Z",
     "start_time": "2026-01-20T08:45:14.367567100Z"
    }
   },
   "outputs": [],
   "source": [
    "class QuantumRNN(Model):\n",
    "    def __init__(self, hidden_units, output_dim, dropout_rate=0.2):\n",
    "        super(QuantumRNN, self).__init__()\n",
    "\n",
    "        # Layer di Normalizzazione\n",
    "        self.norm = layers.LayerNormalization()\n",
    "\n",
    "        # GRU 1: return_sequences=True per passare la sequenza alla seconda GRU\n",
    "        # Aggiungiamo dropout e recurrent_dropout per regolarizzazione\n",
    "        self.gru1 = layers.GRU(\n",
    "            hidden_units,\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        # GRU 2: Elabora la sequenza e restituisce solo l'ultimo stato\n",
    "        self.gru2 = layers.GRU(\n",
    "            hidden_units,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        # Dense Layer finale per la regressione (56 parametri)\n",
    "        self.dense_out = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Il parametro 'training' gestisce automaticamente il dropout:\n",
    "        # viene attivato solo durante fit/train e disattivato durante la predizione\n",
    "        x = self.norm(inputs)\n",
    "        x = self.gru1(x, training=training)\n",
    "        x = self.gru2(x, training=training)\n",
    "        return self.dense_out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4f593",
   "metadata": {},
   "source": [
    "## Architettura Transformer (Time Series)\n",
    "\n",
    "Questo blocco implementa un modello **Transformer Encoder-only**, adattato per la previsione di serie temporali. A differenza delle RNN, che processano i dati sequenzialmente, il Transformer elabora l'intera finestra temporale in parallelo grazie al meccanismo di attenzione.\n",
    "\n",
    "Il codice è suddiviso in due classi:\n",
    "\n",
    "### 1. `TransformerBlock`\n",
    "Rappresenta l'unità fondamentale dell'architettura. Ogni blocco applica le seguenti trasformazioni:\n",
    "* **Multi-Head Self-Attention**: Permette al modello di mettere in relazione ogni passo temporale con tutti gli altri all'interno della finestra, catturando dipendenze a lungo raggio indipendentemente dalla distanza.\n",
    "* **Feed Forward Network (FFN)**: Una rete densa a due strati che elabora le feature estratte dall'attenzione.\n",
    "* **Add & Norm**: L'uso di connessioni residuali ($x + f(x)$) seguite da *Layer Normalization* è cruciale per stabilizzare i gradienti e permettere l'addestramento di reti profonde.\n",
    "\n",
    "### 2. `QuantumTransformer`\n",
    "È la classe che assembla i componenti:\n",
    "1.  **Input Projection**: Un layer `Dense` proietta le 55 feature originali nello spazio latente di dimensione `embed_dim`.\n",
    "2.  **Learnable Positional Encoding**: Poiché il meccanismo di attenzione è invariante all'ordine, viene sommato un vettore di pesi **addestrabili** (`pos_encoding`) all'input per fornire al modello informazioni sulla posizione temporale relativa.\n",
    "3.  **Encoder Stack**: Una sequenza di `num_layers` blocchi Transformer ripetuti.\n",
    "4.  **Global Average Pooling**: Invece di utilizzare solo l'ultimo step temporale, qui viene calcolata la media delle feature su tutta la sequenza temporale. Questo rende la predizione più robusta al rumore dei singoli step.\n",
    "5.  **Output Head**: Un ultimo layer denso proietta il risultato verso la dimensione di output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cbf16d5a2d8b76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T08:45:09.637265700Z",
     "start_time": "2026-01-20T08:45:09.618423900Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseAttention(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(**kwargs)\n",
    "        self.layerNorm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.add = layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context, training=False):\n",
    "        attn_output = self.mha(query=x,value=context,key=context, training=training)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layerNorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(query=x,value=x,key=x)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layerNorm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class CausalSelfAttention(BaseAttention):\n",
    "    def call(self, x, training=False):\n",
    "        attn_output = self.mha(query=x,value=x,key=x, use_causal_mask=True, training=training)\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layerNorm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class FeedForward(layers.Layer):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation=\"relu\"),\n",
    "            layers.Dense(d_model),\n",
    "            layers.Dropout(dropout)\n",
    "        ])\n",
    "        self.layerNorm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, x):\n",
    "        ffn_output = self.seq(x)\n",
    "\n",
    "        x = self.add([x, ffn_output])\n",
    "        x = self.layerNorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LearnablePositionalEncoding(layers.Layer):\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_emb = self.add_weight(\n",
    "            name=\"pos_emb\",\n",
    "            shape=(1, seq_len, d_model),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_emb\n",
    "    \n",
    "class EncoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = SelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.feed_forward = FeedForward(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def call(self, x, attn_mask=None, training=False):\n",
    "        x = self.self_attention(x)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.feed_forward = FeedForward(\n",
    "            d_model=d_model,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def call(self, x, enc_out, training=False):\n",
    "        x = self.causal_self_attention(x, training=training)\n",
    "        x = self.cross_attention(x, enc_out, training=training)\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "class QuantumTransformer(Model):\n",
    "    def __init__(self, input_dim, seq_len, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.enc_input_proj = layers.Dense(d_model)\n",
    "        self.dec_input_proj = layers.Dense(d_model)\n",
    "\n",
    "        self.enc_pos = LearnablePositionalEncoding(seq_len, d_model)\n",
    "        self.dec_pos = LearnablePositionalEncoding(seq_len, d_model)\n",
    "\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.output_head = layers.Dense(input_dim)\n",
    "\n",
    "    def call(self, encoder_input, decoder_input, training=False):\n",
    "        \"\"\"\n",
    "        encoder_input: (batch, T, input_dim)\n",
    "        decoder_input: (batch, T, input_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_output = self.enc_input_proj(encoder_input)\n",
    "        encoder_output = self.enc_pos(encoder_output)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output)\n",
    "\n",
    "        decoder_output = self.dec_input_proj(decoder_input)\n",
    "        decoder_output = self.dec_pos(decoder_output)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(\n",
    "                decoder_output,\n",
    "                encoder_output,\n",
    "                training=training\n",
    "            )\n",
    "        \n",
    "        return self.output_head(decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9c7be",
   "metadata": {},
   "source": [
    "## Pipeline Dati e Data Augmentation\n",
    "\n",
    "In questo blocco vengono definite le funzioni per trasformare i dati grezzi in oggetti `tf.data.Dataset` ottimizzati per l'addestramento e le utility per l'augmentation.\n",
    "\n",
    "**1. Generazione del Dataset (`get_dataset_for_config`)**\n",
    "Questa funzione automatizza la creazione dei batch per il training e il testing in base alla configurazione corrente (es. lunghezza della finestra temporale).\n",
    "* **Sliding Window:** Vengono chiamate le funzioni di creazione dataset per generare le coppie Input ($X$) -> Output ($y$).\n",
    "* **Squeeze:** `y_tr.squeeze(1)` rimuove dimensioni singole superflue, allineando le dimensioni del target con l'output del layer Dense finale.\n",
    "* **Ottimizzazione `tf.data`:**\n",
    "    * `from_tensor_slices`: Converte i numpy array in tensori.\n",
    "    * `shuffle(1024)`: Mescola i dati di training (buffer size 1024) per rompere le correlazioni temporali tra batch consecutivi e garantire l'assunzione dei dati indipendenti e identicamente distribuiti.\n",
    "    * `batch(batch_size)`: Raggruppa i dati in mini-batch.\n",
    "    * `prefetch(tf.data.AUTOTUNE)`: Prepara il batch successivo in background mentre la GPU elabora quello corrente, eliminando i colli di bottiglia nel caricamento dati.\n",
    "\n",
    "**2. Tecniche di Augmentation**\n",
    "Vengono definite due funzioni per aumentare la robustezza del modello, simulando disturbi realistici sui dati di input:\n",
    "* **`apply_masking` (Input Dropout):** Azzera casualmente (con probabilità 15%) alcuni valori della traiettoria in input. Questo costringe la rete a non affidarsi a singole feature ma a imparare il contesto globale, simulando la perdita di pacchetti o dati mancanti.\n",
    "* **`apply_noise` (Gaussian Injection):** Aggiunge rumore bianco gaussiano ($\\mu=0, \\sigma=0.05$) ai dati. Questo agisce come regolarizzatore, impedendo al modello di fare *overfitting* su pattern di rumore specifici del training set e migliorando la generalizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f4d224b3d9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_for_config(window_size, batch_size):\n",
    "    \"\"\"Genera il dataset in base alla finestra temporale della config\"\"\"\n",
    "    # Creazione numpy\n",
    "    X_tr, y_tr = create_dataset(train_traj_norm, input_width=window_size)\n",
    "    X_te, y_te = create_dataset(test_traj_norm, input_width=window_size)\n",
    "\n",
    "    # Squeeze target\n",
    "    y_tr = y_tr.squeeze(1)\n",
    "    y_te = y_te.squeeze(1)\n",
    "\n",
    "    # Creazione tf.data\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_tr, y_tr))\n",
    "    train_ds = train_ds.shuffle(1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_te, y_te)).batch(batch_size)\n",
    "    return train_ds, test_ds, X_te, y_te\n",
    "\n",
    "# Funzioni di Augmentation\n",
    "def apply_masking(x, prob=0.15):\n",
    "    mask = tf.random.uniform(shape=tf.shape(x)) > prob\n",
    "    return x * tf.cast(mask, dtype=x.dtype)\n",
    "\n",
    "def apply_noise(x, stddev=0.05):\n",
    "    noise = tf.random.normal(shape=tf.shape(x), mean=0.0, stddev=stddev, dtype=x.dtype)\n",
    "    return x + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4168235",
   "metadata": {},
   "source": [
    "## Configurazione Sperimentale: Modelli Ricorrenti\n",
    "\n",
    "In questo blocco vengono definiti tre scenari distinti per testare come la complessità del modello e la lunghezza della memoria storica influenzano le prestazioni. La strategia segue un approccio incrementale:\n",
    "\n",
    "| Configurazione | Window | Units | Dropout | LR ($\\eta$) | Descrizione |\n",
    "| :--- | :---: | :---: | :---: | :---: | :--- |\n",
    "| **Config_A (Small)** | 10 | 32 | 0.1 | 0.01 | **Modello Leggero**: Bassa capacità, alto learning rate. Serve per verificare rapidamente se la rete apprende (sanity check). Rischio di *underfitting*. |\n",
    "| **Config_B (Medium)** | 10 | 64 | 0.2 | 0.001 | **Baseline**: Configurazione bilanciata con capacità standard e dropout moderato. È il punto di riferimento. |\n",
    "| **Config_C (Large)** | **20** | **128** | **0.3** | **0.0005** | **Modello Complesso**: Raddoppia la finestra temporale (memoria più lunga) e le unità. Richiede un dropout più alto per prevenire l'*overfitting* e un learning rate basso per una convergenza stabile. |\n",
    "\n",
    "Questa griglia permette di valutare il trade-off tra capacità computazionale e generalizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b14461429bd79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:37:12.387825400Z",
     "start_time": "2026-01-20T14:37:12.348809100Z"
    }
   },
   "outputs": [],
   "source": [
    "HYPERPARAMETERS_LIST = [\n",
    "    {\n",
    "        \"name\": \"Config_A (Small)\",\n",
    "        \"window_size\": 10,\n",
    "        \"units\": 32,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"batch_size\": 64\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Config_B (Medium - Standard)\",\n",
    "        \"window_size\": 10,\n",
    "        \"units\": 64,\n",
    "        \"dropout\": 0.2,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 64\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Config_C (Large - Long Memory)\",\n",
    "        \"window_size\": 20,\n",
    "        \"units\": 128,\n",
    "        \"dropout\": 0.3,\n",
    "        \"learning_rate\": 0.0005,\n",
    "        \"batch_size\": 128\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cf33ee",
   "metadata": {},
   "source": [
    "## Loop di Training Sperimentale e Curriculum Learning\n",
    "\n",
    "Questo blocco rappresenta il cuore dell'esperimento. Invece di usare il metodo standard `.fit()` di Keras, viene implementato un **Custom Training Loop**. Questo approccio offre il massimo controllo sul processo di addestramento, permettendo di modificare dinamicamente le strategie di augmentation in base all'epoca (Curriculum Learning).\n",
    "\n",
    "**1. Configurazione del Training (`run_experiment`)**\n",
    "* **Preparazione:** Inizializza il dataset e il modello `QuantumRNN` utilizzando i parametri specifici del dizionario `config`.\n",
    "* **Ottimizzatore e Loss:** Viene usato **Adam** e la **Mean SquaredError (MSE)**, standard per problemi di regressione.\n",
    "* **Metriche:** Vengono istanziate metriche stateful (`Mean`, `MAE`) per tracciare loss e errore medio assoluto accumulando i risultati batch dopo batch.\n",
    "\n",
    "**2. Custom Steps con `@tf.function`**\n",
    "Le funzioni `train_step` e `test_step` sono decorate con `@tf.function`. Questo compila il codice Python in un **Grafo TensorFlow** statico, garantendo prestazioni molto superiori rispetto all'esecuzione eager standard.\n",
    "* **GradientTape:** Registra le operazioni per il calcolo automatico del gradiente.\n",
    "* **Gestione Fasi:** All'interno del `train_step`, viene applicata l'augmentation (`masking` o `noise`) condizionalmente in base alla fase corrente.\n",
    "\n",
    "**3. Strategia di Curriculum Learning**\n",
    "Il loop delle epoche implementa una strategia a fasi per rendere il modello più robusto progressivamente:\n",
    "1.  **Epoche 0-9 (Standard):** Training su dati puliti per imparare la dinamica base.\n",
    "2.  **Epoche 10-19 (Masking):** Training con input mascherati per forzare il modello a inferire i dati mancanti.\n",
    "3.  **Epoche 20+ (Noise):** Training con iniezione di rumore per migliorare la generalizzazione e stabilizzare i pesi.\n",
    "\n",
    "**4. Gestione Risorse e Main Loop**\n",
    "Il blocco finale itera su `HYPERPARAMETERS_LIST` per testare diverse configurazioni.\n",
    "* **Garbage Collection:** `tf.keras.backend.clear_session()` e `gc.collect()` sono fondamentali quando si addestrano molti modelli in sequenza nello stesso notebook. Puliscono la memoria GPU/RAM dai vecchi grafi computazionali, prevenendo errori di *Out Of Memory (OOM)*.\n",
    "* **Salvataggio:** Al termine di ogni esperimento, vengono salvati:\n",
    "    * La **storia** delle metriche in CSV (per i grafici successivi).\n",
    "    * I **pesi** del modello in formato `.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc4b2890acb811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T08:45:53.388625600Z",
     "start_time": "2026-01-20T08:45:23.426941800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_experiment(config):\n",
    "    print(f\"\\n--- Inizio Training RNN: {config['name']} ---\")\n",
    "\n",
    "    # 1. Preparazione Dati\n",
    "    train_ds, test_ds, _, _ = get_dataset_for_config(config['window_size'], config['batch_size'])\n",
    "\n",
    "    # 2. Creazione Modello\n",
    "    model = QuantumRNN(hidden_units=config['units'], output_dim=55, dropout_rate=config['dropout'])\n",
    "\n",
    "    # 3. Setup Training\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    train_loss_metric = tf.keras.metrics.Mean()\n",
    "    test_loss_metric = tf.keras.metrics.Mean()\n",
    "    train_mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x, y, phase):\n",
    "        if phase == \"masking\": x = apply_masking(x)\n",
    "        elif phase == \"noise\": x = apply_noise(x)\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x, training=True)\n",
    "            loss = loss_fn(y, preds)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss_metric.update_state(loss)\n",
    "        train_mae_metric.update_state(y, preds)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(x, y):\n",
    "        preds = model(x, training=False)\n",
    "        loss = loss_fn(y, preds)\n",
    "        test_loss_metric.update_state(loss)\n",
    "\n",
    "    # 4. Loop Epoche\n",
    "    history = {'loss': [], 'val_loss': [], 'mae': []}\n",
    "    EPOCHS = 30\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch < 10: phase, desc = \"standard\", \"Standard\"\n",
    "        elif epoch < 20: phase, desc = \"masking\", \"Masking\"\n",
    "        else: phase, desc = \"noise\", \"Noise\"\n",
    "\n",
    "        train_loss_metric.reset_state()\n",
    "        test_loss_metric.reset_state()\n",
    "        train_mae_metric.reset_state()\n",
    "\n",
    "        with tqdm(total=len(train_ds), desc=f\"Ep {epoch+1}/{EPOCHS} [{config['name']}-{desc}]\", unit=\"bt\", leave=False) as pbar:\n",
    "            for x_b, y_b in train_ds:\n",
    "                l = train_step(x_b, y_b, phase)\n",
    "                pbar.set_postfix({'loss': f'{l:.4f}'})\n",
    "                pbar.update(1)\n",
    "\n",
    "        for x_te, y_te in test_ds:\n",
    "            test_step(x_te, y_te)\n",
    "\n",
    "        history['loss'].append(train_loss_metric.result().numpy())\n",
    "        history['val_loss'].append(test_loss_metric.result().numpy())\n",
    "        history['mae'].append(train_mae_metric.result().numpy())\n",
    "\n",
    "    # --- SALVATAGGIO STORIA CSV ---\n",
    "    safe_name = config['name'].replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    pd.DataFrame(history).to_csv(f\"../models/history/rnn_{safe_name}.csv\", index=False)\n",
    "    print(f\"Storia salvata: ../models/history/rnn_{safe_name}.csv\")\n",
    "\n",
    "    return history, model\n",
    "\n",
    "# --- ESECUZIONE MAIN LOOP RNN ---\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "results_rnn = {}\n",
    "\n",
    "for config in HYPERPARAMETERS_LIST:\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    hist, trained_model = run_experiment(config)\n",
    "    results_rnn[config['name']] = hist\n",
    "\n",
    "    # Salvataggio Pesi\n",
    "    safe_name = config['name'].replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    file_name = f\"../models/weights/rnn_{safe_name}.weights.h5\"\n",
    "    trained_model.save_weights(file_name)\n",
    "    print(f\"PESI SALVATI: {file_name}\")\n",
    "\n",
    "print(\"\\nEsecuzione RNN completata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab87f71",
   "metadata": {},
   "source": [
    "## Configurazione Sperimentale: Modelli Transformer\n",
    "\n",
    "Parallelamente ai modelli ricorrenti, vengono testate tre architetture basate su **Attention** (`TRANSFORMER_CONFIGS`). Qui gli iperparametri controllano la profondità e la capacità di astrazione del meccanismo di attenzione.\n",
    "\n",
    "I parametri chiave variati sono:\n",
    "* **`num_heads`**: Il numero di \"teste\" di attenzione parallele. Più teste permettono al modello di focalizzarsi su diversi aspetti della relazione temporale contemporaneamente.\n",
    "* **`num_layers`**: La profondità della rete (numero di blocchi Transformer impilati).\n",
    "* **`embed_dim`**: La dimensione dello spazio latente in cui vengono proiettati i dati.\n",
    "\n",
    "| Config | Heads | Layers | Embed Dim | Note |\n",
    "| :--- | :---: | :---: | :---: | :--- |\n",
    "| **Transf_A** | 2 | 1 | 32 | **Shallow**: Un solo strato, poche teste. Simile a una regressione avanzata con attenzione semplice. |\n",
    "| **Transf_B** | 4 | 2 | 64 | **Intermediate**: Aumenta la capacità di rappresentazione (embedding 64) e la profondità. Bilanciamento tra velocità e performance. |\n",
    "| **Transf_C** | 4 | 3 | 128 | **Deep & Wide**: Modello profondo (3 layer) con finestra temporale estesa ($T=20$). Ideale per catturare dipendenze complesse a lungo termine, ma richiede più dati e regolarizzazione. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75464554439087",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"Transf_A (Small)\",\n",
    "        \"window_size\": 10,\n",
    "        \"embed_dim\": 32,      # Dimensione interna\n",
    "        \"num_heads\": 2,       # Quante \"teste\" guardano i dati\n",
    "        \"ff_dim\": 32,         # Dimensione rete interna al blocco\n",
    "        \"num_layers\": 1,      # Solo 1 blocco (leggero)\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 64\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Transf_B (Medium)\",\n",
    "        \"window_size\": 10,\n",
    "        \"embed_dim\": 64,\n",
    "        \"num_heads\": 4,       # Più attenzione ai dettagli\n",
    "        \"ff_dim\": 64,\n",
    "        \"num_layers\": 2,      # 2 Blocchi sovrapposti\n",
    "        \"dropout\": 0.2,\n",
    "        \"learning_rate\": 0.0005, # LR più basso per stabilità\n",
    "        \"batch_size\": 64\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Transf_C (Large - Deep)\",\n",
    "        \"window_size\": 20,    # Finestra più lunga\n",
    "        \"embed_dim\": 128,\n",
    "        \"num_heads\": 4,\n",
    "        \"ff_dim\": 128,\n",
    "        \"num_layers\": 3,      # 3 Blocchi (Modello profondo)\n",
    "        \"dropout\": 0.3,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"batch_size\": 128\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1dd8a",
   "metadata": {},
   "source": [
    "## Loop di Training: Architettura Transformer\n",
    "\n",
    "Questo blocco adatta la pipeline di training sperimentale (già vista per la RNN) all'architettura **Transformer**. Sebbene la logica di *Curriculum Learning* rimanga identica, ci sono differenze cruciali nella fase di istanziazione e costruzione del modello.\n",
    "\n",
    "**1. Inizializzazione del Modello (`QuantumTransformer`)**\n",
    "Il modello viene istanziato con iperparametri specifici per il meccanismo di *Self-Attention*:\n",
    "* **`num_heads`:** Il numero di \"teste\" di attenzione che permettono al modello di focalizzarsi su diverse parti della sequenza simultaneamente (apprendendo relazioni a lungo raggio).\n",
    "* **`ff_dim`:** La dimensione dello strato Feed-Forward interno al blocco Transformer.\n",
    "* **`embed_dim`:** La dimensione della proiezione dei dati nello spazio latente.\n",
    "\n",
    "**2. Il \"Dummy Pass\" (Costruzione del Grafo)**\n",
    "```python\n",
    "dummy_x = tf.random.uniform((1, config['window_size'], 55))\n",
    "_ = model(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf03003f08004fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T13:32:31.484797300Z",
     "start_time": "2026-01-20T08:46:06.340846700Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_transformer_experiment(config):\n",
    "    print(f\"\\n--- Inizio Training Transformer: {config['name']} ---\")\n",
    "\n",
    "    # 1. Preparazione Dati\n",
    "    train_ds, test_ds, _, _ = get_dataset_for_config(config['window_size'], config['batch_size'])\n",
    "\n",
    "    # 2. Creazione Modello\n",
    "    model = QuantumTransformer(\n",
    "        num_layers=config['num_layers'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        num_heads=config['num_heads'],\n",
    "        ff_dim=config['ff_dim'],\n",
    "        output_dim=55,\n",
    "        input_seq_len=config['window_size'],\n",
    "        dropout_rate=config['dropout']\n",
    "    )\n",
    "\n",
    "    dummy_x = tf.random.uniform((1, config['window_size'], 55))\n",
    "    _ = model(dummy_x)\n",
    "\n",
    "    # 3. Setup Training\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    train_loss_metric = tf.keras.metrics.Mean()\n",
    "    test_loss_metric = tf.keras.metrics.Mean()\n",
    "    train_mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x, y, phase):\n",
    "        if phase == \"masking\": x = apply_masking(x)\n",
    "        elif phase == \"noise\": x = apply_noise(x)\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x, training=True)\n",
    "            loss = loss_fn(y, preds)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss_metric.update_state(loss)\n",
    "        train_mae_metric.update_state(y, preds)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(x, y):\n",
    "        preds = model(x, training=False)\n",
    "        loss = loss_fn(y, preds)\n",
    "        test_loss_metric.update_state(loss)\n",
    "\n",
    "    # 4. Loop Epoche\n",
    "    history = {'loss': [], 'val_loss': [], 'mae': []}\n",
    "    EPOCHS = 30\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        if epoch < 10: phase, desc = \"standard\", \"Standard\"\n",
    "        elif epoch < 20: phase, desc = \"masking\", \"Masking\"\n",
    "        else: phase, desc = \"noise\", \"Noise\"\n",
    "\n",
    "        train_loss_metric.reset_state()\n",
    "        test_loss_metric.reset_state()\n",
    "        train_mae_metric.reset_state()\n",
    "\n",
    "        with tqdm(total=len(train_ds), desc=f\"Ep {epoch+1}/{EPOCHS} [{config['name']}-{desc}]\", unit=\"bt\", leave=False) as pbar:\n",
    "            for x_b, y_b in train_ds:\n",
    "                l = train_step(x_b, y_b, phase)\n",
    "                pbar.set_postfix({'loss': f'{l:.4f}'})\n",
    "                pbar.update(1)\n",
    "\n",
    "        for x_te, y_te in test_ds:\n",
    "            test_step(x_te, y_te)\n",
    "\n",
    "        history['loss'].append(train_loss_metric.result().numpy())\n",
    "        history['val_loss'].append(test_loss_metric.result().numpy())\n",
    "        history['mae'].append(train_mae_metric.result().numpy())\n",
    "\n",
    "    # --- SALVATAGGIO STORIA CSV ---\n",
    "    safe_name = config['name'].replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    pd.DataFrame(history).to_csv(f\"../models/history/transf_{safe_name}.csv\", index=False)\n",
    "    print(f\"Storia salvata: ../models/history/transf_{safe_name}.csv\")\n",
    "\n",
    "    return history, model\n",
    "\n",
    "# --- ESECUZIONE MAIN LOOP TRANSFORMER ---\n",
    "results_transf = {}\n",
    "\n",
    "for config in TRANSFORMER_CONFIGS:\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    hist, trained_model = run_transformer_experiment(config)\n",
    "    results_transf[config['name']] = hist\n",
    "\n",
    "    # Salvataggio Pesi\n",
    "    safe_name = config['name'].replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    file_name = f\"../models/weights/transf_{safe_name}.weights.h5\"\n",
    "    trained_model.save_weights(file_name)\n",
    "    print(f\"PESI SALVATI: {file_name}\")\n",
    "\n",
    "print(\"\\nEsecuzione Transformer completata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482ad382441e479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:34:05.569275400Z",
     "start_time": "2026-01-20T14:34:05.552155500Z"
    }
   },
   "outputs": [],
   "source": [
    "# GRAFICI\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "def plot_training_phases_detailed(history, config_name, filename):\n",
    "    \"\"\"\n",
    "    Plotta Loss e MAE con linee verticali allineate perfettamente alle epoche 10 e 20.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "\n",
    "    # Creiamo una figura con 2 grafici affiancati\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # --- GRAFICO 1: LOSS (MSE) ---\n",
    "    ax1.plot(epochs, history['loss'], label='Train Loss', color='#1f77b4', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], label='Validation Loss', color='#ff7f0e', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Calcolo posizione testo\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    text_y_pos = y_max - (y_max - y_min) * 0.05\n",
    "\n",
    "    # --- MODIFICA QUI: Linee verticali su interi esatti ---\n",
    "\n",
    "    # Fase 1: Standard (Testo centrato su epoca 5)\n",
    "    ax1.text(5, text_y_pos, 'FASE 1:\\nSTANDARD', ha='center', va='top', fontsize=10, fontweight='bold', color='gray', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.9))\n",
    "\n",
    "    # Linea su Epoca 10 (Fine Standard / Inizio Masking)\n",
    "    ax1.axvline(x=10, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "    # Fase 2: Masking (Testo centrato su epoca 15)\n",
    "    ax1.text(15, text_y_pos, 'FASE 2:\\nMASKING', ha='center', va='top', fontsize=10, fontweight='bold', color='gray', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.9))\n",
    "\n",
    "    # Linea su Epoca 20 (Fine Masking / Inizio Noise)\n",
    "    ax1.axvline(x=20, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "    # Fase 3: Noise (Testo centrato su epoca 25)\n",
    "    ax1.text(25, text_y_pos, 'FASE 3:\\nNOISE', ha='center', va='top', fontsize=10, fontweight='bold', color='gray', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.9))\n",
    "\n",
    "    ax1.set_title(f'Training Dynamics - {config_name}', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoche')\n",
    "    ax1.set_ylabel('Loss (MSE)')\n",
    "    ax1.legend(loc='lower left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Impostiamo i tick dell'asse X per mostrare i numeri chiave\n",
    "    # Questo forza il grafico a mostrare 1, 10, 20, 30 sull'asse\n",
    "    ax1.set_xticks([1, 5, 10, 15, 20, 25, 30])\n",
    "\n",
    "    # --- GRAFICO 2: MAE ---\n",
    "    ax2.plot(epochs, history['mae'], label='Train MAE', color='#2ca02c', linewidth=2)\n",
    "\n",
    "    # Linee verticali anche qui (esattamente su 10 e 20)\n",
    "    ax2.axvline(x=10, color='red', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(x=20, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    ax2.set_title('Mean Absolute Error Evolution', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoche')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_xticks([1, 5, 10, 15, 20, 25, 30]) # Forza i tick anche qui\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../plots/training/\" + filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_trajectory_check(model, trajectories_list, window_size, config_name, filename):\n",
    "    \"\"\"\n",
    "    Confronto Predizione vs Realtà su una traiettoria di test.\n",
    "    \"\"\"\n",
    "    # Prendiamo una traiettoria di test a caso (es. indice 0)\n",
    "    traj_idx = 1\n",
    "    if len(trajectories_list) > 0:\n",
    "        real_traj = trajectories_list[traj_idx]\n",
    "\n",
    "        # Creazione input sequenziale\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(len(real_traj) - window_size):\n",
    "            X_seq.append(real_traj[i : i + window_size])\n",
    "            y_seq.append(real_traj[i + window_size])\n",
    "\n",
    "        X_seq = np.array(X_seq)\n",
    "        y_seq = np.array(y_seq)\n",
    "\n",
    "        # Predizione\n",
    "        print(\"Generazione predizioni per il grafico...\")\n",
    "        y_pred = model.predict(X_seq, batch_size=32, verbose=0)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        feature_idx = 0 # Magnetizzazione Z (Feature più importante)\n",
    "\n",
    "        plt.plot(y_seq[:, feature_idx], label='Realtà (Ground Truth)', color='black', alpha=0.7, linewidth=2)\n",
    "        plt.plot(y_pred[:, feature_idx], label=f'Predizione ({config_name})', color='#d62728', linestyle='--', linewidth=1.5)\n",
    "\n",
    "        plt.title(f'Verifica Traiettoria: {config_name} (Window: {window_size})', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Valore Normalizzato')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(\"../plots/predictions/\" + filename, dpi=300)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Errore: Lista traiettorie vuota.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURAZIONE PERCORSI ---\n",
    "MODEL_DIR = '../models'\n",
    "PLOT_DIR = '../plots'\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "def run_full_visualization_cycle():\n",
    "    # Definiamo i gruppi di configurazioni da processare\n",
    "    config_groups = {\n",
    "        \"rnn\": HYPERPARAMETERS_LIST,\n",
    "        \"transf\": TRANSFORMER_CONFIGS\n",
    "    }\n",
    "\n",
    "    for m_type, configs in config_groups.items():\n",
    "        for conf in configs:\n",
    "            name = conf['name']\n",
    "            safe_name = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            \n",
    "            # 1. RECUPERO STORIA (CSV) E GRAFICI LOSS/MAE\n",
    "            csv_path = f\"{MODEL_DIR}/history/{m_type}_{safe_name}.csv\"\n",
    "            if os.path.exists(csv_path):\n",
    "                print(f\"\\nGenerando grafici di addestramento per: {name}\")\n",
    "                history_df = pd.read_csv(csv_path)\n",
    "                # Convertiamo il dataframe in dizionario per le tue funzioni\n",
    "                history_dict = history_df.to_dict(orient='list')\n",
    "                \n",
    "                plot_filename = f\"loss_{m_type}_{safe_name}.png\"\n",
    "                plot_training_phases_detailed(history_dict, name, plot_filename)\n",
    "            else:\n",
    "                print(f\"Storia non trovata per {name} al percorso: {csv_path}\")\n",
    "\n",
    "            # 2. RICARICAMENTO MODELLO E GRAFICI PREDIZIONE\n",
    "            weights_path = f\"{MODEL_DIR}/weights/{m_type}_{safe_name}.weights.h5\"\n",
    "            if os.path.exists(weights_path):\n",
    "                print(f\"Generando grafici di predizione per: {name}\")\n",
    "                \n",
    "                # Istanziamo l'architettura corretta\n",
    "                if m_type == \"rnn\":\n",
    "                    model = QuantumRNN(hidden_units=conf['units'], output_dim=55, dropout_rate=conf['dropout'])\n",
    "                else:\n",
    "                    model = QuantumTransformer(\n",
    "                        num_layers=conf['num_layers'], embed_dim=conf['embed_dim'],\n",
    "                        num_heads=conf['num_heads'], ff_dim=conf['ff_dim'],\n",
    "                        output_dim=55, input_seq_len=conf['window_size'], dropout_rate=conf['dropout']\n",
    "                    )\n",
    "                \n",
    "                # Dummy pass per inizializzare i pesi e caricamento\n",
    "                dummy_input = tf.random.uniform((1, conf['window_size'], 55))\n",
    "                _ = model(dummy_input)\n",
    "                model.load_weights(weights_path)\n",
    "                \n",
    "                pred_filename = f\"pred_{m_type}_{safe_name}.png\"\n",
    "                # Assicurati che test_traj_norm sia disponibile nel tuo ambiente\n",
    "                plot_trajectory_check(model, test_traj_norm, conf['window_size'], name, pred_filename)\n",
    "                \n",
    "                # Pulizia memoria dopo ogni modello per evitare crash\n",
    "                tf.keras.backend.clear_session()\n",
    "            else:\n",
    "                print(f\"Pesi non trovati per {name} al percorso: {weights_path}\")\n",
    "\n",
    "# Avvia il ciclo\n",
    "run_full_visualization_cycle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
