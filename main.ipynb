{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8467df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Aggiunge la cartella src al path per poter importare i moduli\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "\n",
    "# Autoreload per ricaricare i moduli se li modifichi durante lo sviluppo\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import dei tuoi moduli custom\n",
    "from data_loader import QuantumDataManager\n",
    "from quantum_transformer import QuantumTransformer\n",
    "from curriculum import CurriculumLearning\n",
    "from utils import ExperimentManager "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932f4d224b3d9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRANSFORMER_CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"transformer_small\",\n",
    "        \"window_size\": 10,\n",
    "        \"forecast_horizon\": 1,\n",
    "        \"embed_dim\": 32,\n",
    "        \"num_heads\": 2,\n",
    "        \"ff_dim\": 128,\n",
    "        \"num_layers\": 1,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 64\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"transformer_medium\",\n",
    "        \"window_size\": 15,\n",
    "        \"forecast_horizon\": 1,\n",
    "        \"embed_dim\": 64,\n",
    "        \"num_heads\": 4,\n",
    "        \"ff_dim\": 256,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.2,\n",
    "        \"learning_rate\": 0.0005,\n",
    "        \"batch_size\": 64\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"transformer_large\",\n",
    "        \"window_size\": 30,\n",
    "        \"forecast_horizon\": 1,\n",
    "        \"embed_dim\": 64,\n",
    "        \"num_heads\": 4,\n",
    "        \"ff_dim\": 256,\n",
    "        \"num_layers\": 3,\n",
    "        \"dropout\": 0.15,       \n",
    "        \"learning_rate\": 0.0003,\n",
    "        \"batch_size\": 128\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e552f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_curriculum(\n",
    "    model,\n",
    "    train_ds,\n",
    "    val_ds,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    epochs,\n",
    "    tf_epochs=5,\n",
    "    mm_epochs=8,\n",
    "    ss_epochs=2,\n",
    "    ss_max_prob=0.8\n",
    "):\n",
    "    curriculum = CurriculumLearning(model, optimizer, loss_fn)\n",
    "\n",
    "    history = {\n",
    "        \"loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"phase\": []\n",
    "    }\n",
    "\n",
    "    total_epochs = tf_epochs + mm_epochs + ss_epochs\n",
    "    assert epochs == total_epochs\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- 1. Determina la fase ---\n",
    "        if epoch < tf_epochs:\n",
    "            phase = \"teacher_forcing\"\n",
    "            sampling_prob = 0.0\n",
    "        elif epoch < tf_epochs + mm_epochs:\n",
    "            phase = \"masked_modeling\"\n",
    "            sampling_prob = 0.0\n",
    "        else:\n",
    "            phase = \"scheduled_sampling\"\n",
    "            ss_progress = (epoch - tf_epochs - mm_epochs) / max(1, ss_epochs - 1)\n",
    "            sampling_prob = ss_progress * ss_max_prob\n",
    "\n",
    "        # --- 2. Training Loop ---\n",
    "        epoch_loss = tf.keras.metrics.Mean()\n",
    "        \n",
    "        for x, y in train_ds:\n",
    "            if phase == \"teacher_forcing\":\n",
    "                loss = curriculum.teacher_forcing_step(x, y)\n",
    "            elif phase == \"masked_modeling\":\n",
    "                loss = curriculum.masked_modeling_step(x, y)\n",
    "            else:\n",
    "                loss = curriculum.scheduled_sampling_step(x, y, sampling_prob=sampling_prob)\n",
    "            \n",
    "            epoch_loss.update_state(loss)\n",
    "\n",
    "        # --- 3. Validation Loop (NUOVO) ---\n",
    "        # Calcoliamo la loss sul validation set (in modalità Teacher Forcing standard)\n",
    "        val_loss_metric = tf.keras.metrics.Mean()\n",
    "        for x_val, y_val in val_ds:\n",
    "            preds = model(encoder_input=x_val, decoder_input=y_val, training=False)\n",
    "            v_loss = loss_fn(y_val, preds)\n",
    "            val_loss_metric.update_state(v_loss)\n",
    "\n",
    "        # --- 4. Logging ---\n",
    "        train_l = epoch_loss.result().numpy()\n",
    "        val_l = val_loss_metric.result().numpy()\n",
    "        \n",
    "        history[\"loss\"].append(train_l)\n",
    "        history[\"val_loss\"].append(val_l)\n",
    "        history[\"phase\"].append(phase)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d} | {phase[:15]:15s} | Train Loss: {train_l:.5f} | Val Loss: {val_l:.5f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Avvio Esperimento: transformer_small ---\n",
      "Dataset caricato: (400400, 56)\n",
      "Traiettorie individuate: 400\n",
      "[transformer_small] Dataset caricato. Train shape: (317120, 10, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 10:39:33.593571: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2026-01-27 10:40:33.738366: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2026-01-27 10:40:52.359199: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | teacher_forcing | Train Loss: 0.01898 | Val Loss: 0.01730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 10:42:11.128412: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 | teacher_forcing | Train Loss: 0.01231 | Val Loss: 0.01679\n",
      "Epoch 003 | teacher_forcing | Train Loss: 0.01220 | Val Loss: 0.01619\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Esecuzione\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mrun_curriculum_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRANSFORMER_CONFIGS\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[33;03m'''for transformer_config in TRANSFORMER_CONFIGS:\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    run_curriculum_experiment(transformer_config)'''\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mrun_curriculum_experiment\u001b[39m\u001b[34m(config, tf_ep, mm_ep, ss_ep)\u001b[39m\n\u001b[32m     28\u001b[39m total_epochs = tf_ep + mm_ep + ss_ep\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 3. Training (con Validation!)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m history = \u001b[43mtrain_with_curriculum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtf_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_ep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_ep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mss_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mss_ep\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 4. Salvataggio e Plotting (tramite la classe ExperimentManager)\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Salvataggio Risultati ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mtrain_with_curriculum\u001b[39m\u001b[34m(model, train_ds, val_ds, optimizer, loss_fn, epochs, tf_epochs, mm_epochs, ss_epochs, ss_max_prob)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     46\u001b[39m         loss = curriculum.scheduled_sampling_step(x, y, sampling_prob=sampling_prob)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[43mepoch_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# --- 3. Validation Loop (NUOVO) ---\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Calcoliamo la loss sul validation set (in modalità Teacher Forcing standard)\u001b[39;00m\n\u001b[32m     52\u001b[39m val_loss_metric = tf.keras.metrics.Mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/keras/src/metrics/reduction_metrics.py:149\u001b[39m, in \u001b[36mMean.update_state\u001b[39m\u001b[34m(self, values, sample_weight)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    148\u001b[39m     num_samples = \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m.\u001b[49m\u001b[43massign_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/keras/src/backend/common/variables.py:297\u001b[39m, in \u001b[36mVariable.assign_add\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34massign_add\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.assign(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/keras/src/backend/common/variables.py:481\u001b[39m, in \u001b[36mVariable.__add__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/keras/src/backend/tensorflow/sparse.py:493\u001b[39m, in \u001b[36melementwise_binary_union.<locals>.wrap_elementwise_binary_union.<locals>.sparse_wrapper\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x2, tf.IndexedSlices):\n\u001b[32m    491\u001b[39m     \u001b[38;5;66;03m# x2 is an IndexedSlices, densify.\u001b[39;00m\n\u001b[32m    492\u001b[39m     x2 = tf.convert_to_tensor(x2)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/keras/src/backend/tensorflow/numpy.py:131\u001b[39m, in \u001b[36madd\u001b[39m\u001b[34m(x1, x2)\u001b[39m\n\u001b[32m    128\u001b[39m         x2 = tf.squeeze(x2)\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tf.nn.bias_add(x1, x2, data_format=data_format)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[39m, in \u001b[36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    141\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m    144\u001b[39m   bound_arguments.apply_defaults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[39m, in \u001b[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1258\u001b[39m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1260\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1262\u001b[39m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1264\u001b[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py:3935\u001b[39m, in \u001b[36madd\u001b[39m\u001b[34m(x, y, name)\u001b[39m\n\u001b[32m   3866\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns x + y element-wise.\u001b[39;00m\n\u001b[32m   3867\u001b[39m \n\u001b[32m   3868\u001b[39m \u001b[33;03mExample usages below.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3932\u001b[39m \u001b[33;03m  name: A name for the operation (optional)\u001b[39;00m\n\u001b[32m   3933\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3934\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(name, \u001b[33m\"\u001b[39m\u001b[33mAdd\u001b[39m\u001b[33m\"\u001b[39m, [x]) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[32m-> \u001b[39m\u001b[32m3935\u001b[39m   x = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3936\u001b[39m   y = ops.convert_to_tensor(y, dtype_hint=x.dtype.base_dtype, name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3937\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m x.dtype == dtypes.string:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[39m, in \u001b[36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, **trace_kwargs):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:732\u001b[39m, in \u001b[36mconvert_to_tensor\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[32m    731\u001b[39m preferred_dtype = preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    225\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    226\u001b[39m           _add_error_prefix(\n\u001b[32m    227\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m for type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m               \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret.dtype.base_dtype.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    231\u001b[39m               name=name))\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m   ret = \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    237\u001b[39m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:2375\u001b[39m, in \u001b[36m_dense_var_to_tensor\u001b[39m\u001b[34m(var, dtype, name, as_ref)\u001b[39m\n\u001b[32m   2374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_dense_var_to_tensor\u001b[39m(var, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m, as_ref=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2375\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvar\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_dense_var_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:1621\u001b[39m, in \u001b[36mBaseResourceVariable._dense_var_to_tensor\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1619\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_value().op.inputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1620\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:656\u001b[39m, in \u001b[36mBaseResourceVariable.value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    654\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_value\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:841\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op\u001b[39m\u001b[34m(self, no_copy)\u001b[39m\n\u001b[32m    839\u001b[39m       result = read_and_set_handle(no_copy)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m   result = \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context.executing_eagerly():\n\u001b[32m    844\u001b[39m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[32m    845\u001b[39m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[32m    846\u001b[39m   record.record_operation(\n\u001b[32m    847\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mReadVariableOp\u001b[39m\u001b[33m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m.handle],\n\u001b[32m    848\u001b[39m       backward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[32m    849\u001b[39m       forward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:833\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[39m\u001b[34m(no_copy)\u001b[39m\n\u001b[32m    830\u001b[39m   gen_resource_variable_ops.disable_copy_on_read(\u001b[38;5;28mself\u001b[39m.handle)\n\u001b[32m    831\u001b[39m result = gen_resource_variable_ops.read_variable_op(\n\u001b[32m    832\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle, \u001b[38;5;28mself\u001b[39m._dtype)\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m \u001b[43m_maybe_set_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/deep/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:319\u001b[39m, in \u001b[36m_maybe_set_handle_data\u001b[39m\u001b[34m(dtype, handle, tensor)\u001b[39m\n\u001b[32m    314\u001b[39m   shape.assert_is_compatible_with(value_tensor.shape)\n\u001b[32m    315\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_resource_variable_ops.assign_variable_op(\n\u001b[32m    316\u001b[39m       handle, value_tensor, name=name)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_set_handle_data\u001b[39m(dtype, handle, tensor):\n\u001b[32m    320\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m dtype == dtypes.variant:\n\u001b[32m    321\u001b[39m     \u001b[38;5;66;03m# For DT_VARIANT types, the handle's shape_and_type[1:] stores the\u001b[39;00m\n\u001b[32m    322\u001b[39m     \u001b[38;5;66;03m# variant's handle data.  Extract it.\u001b[39;00m\n\u001b[32m    323\u001b[39m     handle_data = get_eager_safe_handle_data(handle)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Inizializza il manager\n",
    "exp_manager = ExperimentManager(base_path=\"data\")\n",
    "\n",
    "def run_curriculum_experiment(config, tf_ep=5, mm_ep=10, ss_ep=10):\n",
    "    print(f\"\\n--- Avvio Esperimento: {config['name']} ---\")\n",
    "    \n",
    "    # 1. Caricamento Dati\n",
    "    dm = QuantumDataManager(config)\n",
    "    \n",
    "    # Assicurati che i dati siano caricati per accedere a X_train\n",
    "    if not hasattr(dm, 'X_train'):\n",
    "        dm.load_and_process()\n",
    "\n",
    "    # Leggi la dim da Numpy invece che dal Dataset ---\n",
    "    input_dim = dm.X_train.shape[-1] \n",
    "\n",
    "    # Ora crea i dataset con cache attiva\n",
    "    train_ds, val_ds = dm.get_tf_datasets()\n",
    "    \n",
    "    print(f\"Dati caricati. Input features: {input_dim}\")\n",
    "\n",
    "    # 2. Modello\n",
    "    model = QuantumTransformer(\n",
    "        input_dim=input_dim,\n",
    "        seq_len=config['window_size'],\n",
    "        d_model=config['embed_dim'],\n",
    "        num_heads=config['num_heads'],\n",
    "        d_ff=config['ff_dim'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout']\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    total_epochs = tf_ep + mm_ep + ss_ep\n",
    "\n",
    "    # 3. Training (con Validation!)\n",
    "    history = train_with_curriculum(\n",
    "        model=model,\n",
    "        train_ds=train_ds,\n",
    "        val_ds=val_ds,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        epochs=total_epochs,\n",
    "        tf_epochs=tf_ep,\n",
    "        mm_epochs=mm_ep,\n",
    "        ss_epochs=ss_ep\n",
    "    )\n",
    "    \n",
    "    # 4. Salvataggio e Plotting (tramite la classe ExperimentManager)\n",
    "    print(\"\\n--- Salvataggio Risultati ---\")\n",
    "    exp_manager.save_model_artifacts(model, history, config['name'])\n",
    "    \n",
    "    print(\"\\n--- Generazione Grafici ---\")\n",
    "    exp_manager.plot_loss_curves(history, config['name'])\n",
    "    exp_manager.plot_forecast_comparison(model, val_ds, dm, config['name'])\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Esecuzione\n",
    "run_curriculum_experiment(TRANSFORMER_CONFIGS[0])\n",
    "'''for transformer_config in TRANSFORMER_CONFIGS:\n",
    "    run_curriculum_experiment(transformer_config)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482ad382441e479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T14:34:05.569275400Z",
     "start_time": "2026-01-20T14:34:05.552155500Z"
    }
   },
   "outputs": [],
   "source": [
    "# GRAFICI\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "def plot_training_phases_detailed(history, config_name, filename):\n",
    "    \"\"\"\n",
    "    Plotta Loss e MAE con linee verticali allineate perfettamente alle epoche 10 e 20.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "\n",
    "    # Creiamo una figura con 2 grafici affiancati\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # --- GRAFICO 1: LOSS (MSE) ---\n",
    "    ax1.plot(epochs, history['loss'], label='Train Loss', color='#1f77b4', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], label='Validation Loss', color='#ff7f0e', linestyle='--', linewidth=2)\n",
    "\n",
    "    # Calcolo posizione testo\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    text_y_pos = y_max - (y_max - y_min) * 0.05\n",
    "\n",
    "    # --- MODIFICA QUI: Linee verticali su interi esatti ---\n",
    "\n",
    "    # Fase 1: Standard (Testo centrato su epoca 5)\n",
    "    ax1.text(5, text_y_pos, 'FASE 1:\\nSTANDARD', ha='center', va='top', fontsize=10, fontweight='bold', color='gray', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.9))\n",
    "\n",
    "    # Linea su Epoca 10 (Fine Standard / Inizio Masking)\n",
    "    ax1.axvline(x=10, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "    # Fase 2: Masking (Testo centrato su epoca 15)\n",
    "    ax1.text(15, text_y_pos, 'FASE 2:\\nMASKING', ha='center', va='top', fontsize=10, fontweight='bold', color='gray', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.9))\n",
    "\n",
    "    # Linea su Epoca 20 (Fine Masking / Inizio Noise)\n",
    "    ax1.axvline(x=20, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "    # Fase 3: Noise (Testo centrato su epoca 25)\n",
    "    ax1.text(25, text_y_pos, 'FASE 3:\\nNOISE', ha='center', va='top', fontsize=10, fontweight='bold', color='gray', bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.9))\n",
    "\n",
    "    ax1.set_title(f'Training Dynamics - {config_name}', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoche')\n",
    "    ax1.set_ylabel('Loss (MSE)')\n",
    "    ax1.legend(loc='lower left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Impostiamo i tick dell'asse X per mostrare i numeri chiave\n",
    "    # Questo forza il grafico a mostrare 1, 10, 20, 30 sull'asse\n",
    "    ax1.set_xticks([1, 5, 10, 15, 20, 25, 30])\n",
    "\n",
    "    # --- GRAFICO 2: MAE ---\n",
    "    ax2.plot(epochs, history['mae'], label='Train MAE', color='#2ca02c', linewidth=2)\n",
    "\n",
    "    # Linee verticali anche qui (esattamente su 10 e 20)\n",
    "    ax2.axvline(x=10, color='red', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(x=20, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    ax2.set_title('Mean Absolute Error Evolution', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoche')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_xticks([1, 5, 10, 15, 20, 25, 30]) # Forza i tick anche qui\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../plots/training/\" + filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_trajectory_check(model, trajectories_list, window_size, config_name, filename):\n",
    "    \"\"\"\n",
    "    Confronto Predizione vs Realtà su una traiettoria di test.\n",
    "    \"\"\"\n",
    "    # Prendiamo una traiettoria di test a caso (es. indice 0)\n",
    "    traj_idx = 1\n",
    "    if len(trajectories_list) > 0:\n",
    "        real_traj = trajectories_list[traj_idx]\n",
    "\n",
    "        # Creazione input sequenziale\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(len(real_traj) - window_size):\n",
    "            X_seq.append(real_traj[i : i + window_size])\n",
    "            y_seq.append(real_traj[i + window_size])\n",
    "\n",
    "        X_seq = np.array(X_seq)\n",
    "        y_seq = np.array(y_seq)\n",
    "\n",
    "        # Predizione\n",
    "        print(\"Generazione predizioni per il grafico...\")\n",
    "        y_pred = model.predict(X_seq, batch_size=32, verbose=0)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        feature_idx = 0 # Magnetizzazione Z (Feature più importante)\n",
    "\n",
    "        plt.plot(y_seq[:, feature_idx], label='Realtà (Ground Truth)', color='black', alpha=0.7, linewidth=2)\n",
    "        plt.plot(y_pred[:, feature_idx], label=f'Predizione ({config_name})', color='#d62728', linestyle='--', linewidth=1.5)\n",
    "\n",
    "        plt.title(f'Verifica Traiettoria: {config_name} (Window: {window_size})', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Valore Normalizzato')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(\"../plots/predictions/\" + filename, dpi=300)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Errore: Lista traiettorie vuota.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURAZIONE PERCORSI ---\n",
    "MODEL_DIR = '../models_data'\n",
    "PLOT_DIR = '../plots'\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "def run_full_visualization_cycle():\n",
    "    # Definiamo i gruppi di configurazioni da processare\n",
    "    config_groups = {\n",
    "        \"rnn\": HYPERPARAMETERS_LIST,\n",
    "        \"transf\": TRANSFORMER_CONFIGS\n",
    "    }\n",
    "\n",
    "    for m_type, configs in config_groups.items():\n",
    "        for conf in configs:\n",
    "            name = conf['name']\n",
    "            safe_name = name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            \n",
    "            # 1. RECUPERO STORIA (CSV) E GRAFICI LOSS/MAE\n",
    "            csv_path = f\"{MODEL_DIR}/history/{m_type}_{safe_name}.csv\"\n",
    "            if os.path.exists(csv_path):\n",
    "                print(f\"\\nGenerando grafici di addestramento per: {name}\")\n",
    "                history_df = pd.read_csv(csv_path)\n",
    "                # Convertiamo il dataframe in dizionario per le tue funzioni\n",
    "                history_dict = history_df.to_dict(orient='list')\n",
    "                \n",
    "                plot_filename = f\"loss_{m_type}_{safe_name}.png\"\n",
    "                plot_training_phases_detailed(history_dict, name, plot_filename)\n",
    "            else:\n",
    "                print(f\"Storia non trovata per {name} al percorso: {csv_path}\")\n",
    "\n",
    "            # 2. RICARICAMENTO MODELLO E GRAFICI PREDIZIONE\n",
    "            weights_path = f\"{MODEL_DIR}/weights/{m_type}_{safe_name}.weights.h5\"\n",
    "            if os.path.exists(weights_path):\n",
    "                print(f\"Generando grafici di predizione per: {name}\")\n",
    "                \n",
    "                # Istanziamo l'architettura corretta\n",
    "                if m_type == \"rnn\":\n",
    "                    model = QuantumRNN(hidden_units=conf['units'], output_dim=55, dropout_rate=conf['dropout'])\n",
    "                else:\n",
    "                    model = QuantumTransformer(\n",
    "                        num_layers=conf['num_layers'], embed_dim=conf['embed_dim'],\n",
    "                        num_heads=conf['num_heads'], ff_dim=conf['ff_dim'],\n",
    "                        output_dim=55, input_seq_len=conf['window_size'], dropout_rate=conf['dropout']\n",
    "                    )\n",
    "                \n",
    "                # Dummy pass per inizializzare i pesi e caricamento\n",
    "                dummy_input = tf.random.uniform((1, conf['window_size'], 55))\n",
    "                _ = model(dummy_input)\n",
    "                model.load_weights(weights_path)\n",
    "                \n",
    "                pred_filename = f\"pred_{m_type}_{safe_name}.png\"\n",
    "                # Assicurati che test_traj_norm sia disponibile nel tuo ambiente\n",
    "                plot_trajectory_check(model, test_traj_norm, conf['window_size'], name, pred_filename)\n",
    "                \n",
    "                # Pulizia memoria dopo ogni modello per evitare crash\n",
    "                tf.keras.backend.clear_session()\n",
    "            else:\n",
    "                print(f\"Pesi non trovati per {name} al percorso: {weights_path}\")\n",
    "\n",
    "# Avvia il ciclo\n",
    "run_full_visualization_cycle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
